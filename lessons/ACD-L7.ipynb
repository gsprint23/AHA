{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [AHA! Activity Health Analytics](http://casas.wsu.edu/)\n",
    "[Center for Advanced Studies of Adaptive Systems (CASAS)](http://casas.wsu.edu/)\n",
    "\n",
    "[Washington State University](https://wsu.edu)\n",
    "\n",
    "# L7 Virtual Classifier: Part 2\n",
    "\n",
    "## Learner Objectives\n",
    "At the conclusion of this lesson, participants should have an understanding of:\n",
    "* Finishing implementing steps of PACD with the virtual classifier algorithm\n",
    "    * Class labeling\n",
    "    * Decision tree training\n",
    "    * Change significance testing\n",
    "* Applying the virtual classifier algorithm to synthetic datasets\n",
    "\n",
    "## Acknowledgments\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* [Sprint et al., 2016](http://www.sciencedirect.com/science/article/pii/S1532046416300740)\n",
    "* [Hido et al., 2008](https://link.springer.com/chapter/10.1007%2F978-3-540-68125-0_15)\n",
    "* [sci-kit learn](http://scikit-learn.org) machine learning library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VC Example (Continued)\n",
    "In the previous lesson, wrote code to do the following:\n",
    "1. Read [fitbit_example_steps_df.csv](https://raw.githubusercontent.com/gsprint23/aha/master/lessons/files/fitbit_example_steps_df.csv) into a data frame\n",
    "1. Compute daily features and store them in a data frame\n",
    "1. Write the features data frame to the file [fitbit_example_features_df.csv](https://raw.githubusercontent.com/gsprint23/aha/master/lessons/files/fitbit_example_features_df.csv)\n",
    "\n",
    "We are going to pick up where we left off by reading [fitbit_example_features_df.csv](https://raw.githubusercontent.com/gsprint23/aha/master/lessons/files/fitbit_example_features_df.csv) into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "fname = r\"files\\fitbit_example_data_features_df.csv\"\n",
    "features_df = pd.read_csv(fname, header=0, index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Windows\n",
    "Now that we have feature vectors (rows in our features data frame) for each day, we are ready to label our feature vectors in each window as positive or negative classes:\n",
    "1. $W_{1}$: positive class\n",
    "1. $W_{8}$: negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def label_windows(features_df): \n",
    "    '''\n",
    "    features_df is a data frame with string dates as the index and features as the columns\n",
    "    '''\n",
    "    features_df[\"class\"] = np.NaN\n",
    "    features_df.loc[0:7, \"class\"] = 1\n",
    "    features_df.loc[7:, \"class\"] = -1\n",
    "\n",
    "    W1 = features_df.iloc[0:7]\n",
    "    W2 = features_df.iloc[7:14]\n",
    "    return W1, W2\n",
    "\n",
    "W1, W2 = label_windows(features_df)\n",
    "print(len(W1) == len(W2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Binary Classifier\n",
    "Now that we have each day in our windows labeled as a positive or negative class, it's time to train a binary classifier to discriminate between the windows! To do this, we are going to use the [sci-kit learn](http://scikit-learn.org) machine learning library. Specifically, we will train an instance of `sci-kit learn`'s [`DecisionTreeClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) in K fold cross validation. In sci-kit learn, K fold cross validation is computed with the `cross_val_score()` function, which by default for a `DecisionTreeClassifier` evaluates the model by mean accuracy, which is what we want!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.428571428571\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.tree as tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def compute_VC_change_score(W1, W2):\n",
    "    '''\n",
    "    W1 and W2 are dataframes with string dates as the index and features as columns\n",
    "    includes class as a column\n",
    "    '''\n",
    "    W_concat = pd.concat((W1, W2))\n",
    "    W_concat_Y = W_concat[\"class\"]\n",
    "    W_concat_X = W_concat.drop(\"class\", axis=1)\n",
    "\n",
    "    y = features_df\n",
    "    clf = DecisionTreeClassifier(random_state=0)\n",
    "    cv_scores = cross_val_score(clf, W_concat_X, W_concat_Y, cv=7)\n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "    return mean_cv_score\n",
    "\n",
    "p_VC = compute_VC_change_score(W1, W2)\n",
    "print(p_VC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance Testing\n",
    "Significance testing of change score $CS$ is necessary to interpret change score values. For the VC approach, Hido and colleagues proposed a test of significance to determine if $p_{VC}$ is significantly greater than $p_{rand}$. For this test, the inverse survival function of a binomial distribution is used to determine a critical value, $p_{critical}$, at which $n$ Bernoulli trials are expected to exceed $p_{rand}$ at $\\alpha$ significance. If $p_{VC} \\geq p_{critical}$, a significant change exists between the two windows, $W_{i}$ and $W_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_bin: 0.50 p_VC: 0.43 p_critical: 0.71 is_sig: 0\n"
     ]
    }
   ],
   "source": [
    "def compute_binomial_critical_probability(N1, N2, p_bin, alpha):\n",
    "    '''\n",
    "    https://oneau.wordpress.com/2011/02/28/simple-statistics-with-scipy/\n",
    "    '''\n",
    "    N = N1 + N2\n",
    "    critical_value = sp.stats.binom.isf(alpha, N, p_bin)\n",
    "    critical_probability = critical_value / N\n",
    "    return critical_probability\n",
    "\n",
    "# for equal length arrays, binomial maximum likelihood will be 0.5\n",
    "p_bin = 0.5\n",
    "p_critical = compute_binomial_critical_probability(len(W1), len(W2), p_bin, 0.05)\n",
    "is_sig = 1 if p_VC > p_critical else 0\n",
    "print(\"p_bin: %.2f p_VC: %.2f p_critical: %.2f is_sig: %d\" %(p_bin, p_VC, p_critical, is_sig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate that the VC obtained an average prediction accuracy of 0.43, which is worse than random! The prediction accuracy needs to be greater than or equal to the critical value (0.71), which is not the case: $0.43 \\ngeq 0.71$. We have seen an example where VC does not capture significant changes in the data. Let's take a look at a few examples where VC does capture significant changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Examples\n",
    "To more holistically see how the VC approach captures pattern changes, let's take a look at synthetic Fitbit data. To generate synthetic Fitbit data, we re-sampled step data collected from a volunteer wearing a Fitbit Charge Heart Rate fitness tracker and modified the data to produce four different hybrid-synthetic (HS) physical activity profiles, each exhibiting a different type of change. The length of HS profiles is set to 14 days, resulting in two equal size windows of 7 days for comparison (days 1-7 compared to days 8-14). The HS profiles with their profile identification (HS1-4) and a description are as follows:\n",
    "1. HS1: Medium day-to-day change and consequently significant window-to-window change. Increased bout duration and intensity from day-to-day.\n",
    "1. HS2: No significant day-to-day change but significant window-to-window change. Increased activity for days 7-12.\n",
    "1. HS3: Medium day-to-day change and consequently significant window-to-window change. Increased activity variability from day-to-day.\n",
    "1. HS4: No significant day-to-day change for days 1-6. Significant day-to-day activity variability for days 7-12. Consequently significant window-to-window change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid-Synthetic Profile 1\n",
    "[fitbit_hybrid1_steps_df.csv](https://raw.githubusercontent.com/gsprint23/aha/master/lessons/files/fitbit_hybrid1_steps_df.csv)\n",
    "<img src=\"https://raw.githubusercontent.com/gsprint23/aha/master/lessons/figures/ADM_hybrid1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_bin: 0.50 p_VC: 0.93 p_critical: 0.71 is_sig: 1\n"
     ]
    }
   ],
   "source": [
    "def compute_features_df(df):\n",
    "    '''\n",
    "    df is a dataframe with a DateTimeIndex and columns corresponding to dates\n",
    "    '''\n",
    "    features_df = pd.DataFrame(index=df.columns)\n",
    "    features_df[\"total\"] = df.sum()\n",
    "    features_df[\"max\"] = df.max()\n",
    "    features_df[\"mean\"] = df.mean()\n",
    "    features_df[\"std\"] = df.std()\n",
    "\n",
    "    # PHYSICAL ACTIVITY INTENSITY FEATURES\n",
    "    intensities = [\"sedentary\", \"low\", \"moderate\", \"high\"]\n",
    "    # add blank columns for each intensity. values will be filled in one at a time\n",
    "    for intensity in intensities:\n",
    "        features_df[intensity] = np.NaN\n",
    "    \n",
    "    # the largest step count in the data set\n",
    "    max_val = features_df[\"max\"].max()\n",
    "    # need to adjust if resample data using sum instead of mean\n",
    "    # exclusive of left, inclusive of right\n",
    "    bins = [-1, 4, 39, 99, max_val]\n",
    "\n",
    "    for date in df.columns:\n",
    "        intensity_ser = pd.cut(df[date], bins, labels=intensities)\n",
    "        counts = pd.value_counts(intensity_ser)\n",
    "        for intensity in intensities:\n",
    "            percentage = counts.loc[intensity] / len(df) * 100\n",
    "            features_df.ix[date][intensity] = percentage # use loc because counts is a categorial index\n",
    "    return features_df\n",
    "\n",
    "def run_virtual_classifier(fname):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    df = pd.read_csv(fname, header=0, index_col=[0])\n",
    "    features_df = compute_features_df(df)\n",
    "    W1, W2 = label_windows(features_df)\n",
    "    p_VC = compute_VC_change_score(W1, W2)\n",
    "    p_bin = 0.5\n",
    "    p_critical = compute_binomial_critical_probability(len(W1), len(W2), p_bin, 0.05)\n",
    "    is_sig = 1 if p_VC > p_critical else 0\n",
    "    print(\"p_bin: %.2f p_VC: %.2f p_critical: %.2f is_sig: %d\" %(p_bin, p_VC, p_critical, is_sig))\n",
    "    \n",
    "fname = r\"files\\fitbit_hybrid1_data_steps_df.csv\"\n",
    "run_virtual_classifier(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid-Synthetic Profile 2\n",
    "[fitbit_hybrid2_steps_df.csv](https://raw.githubusercontent.com/gsprint23/aha/master/lessons/files/fitbit_hybrid2_steps_df.csv)\n",
    "<img src=\"https://raw.githubusercontent.com/gsprint23/aha/master/lessons/figures/ADM_hybrid2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_bin: 0.50 p_VC: 1.00 p_critical: 0.71 is_sig: 1\n"
     ]
    }
   ],
   "source": [
    "fname = r\"files\\fitbit_hybrid2_data_steps_df.csv\"\n",
    "run_virtual_classifier(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid-Synthetic Profile 3\n",
    "[fitbit_hybrid3_steps_df.csv](https://raw.githubusercontent.com/gsprint23/aha/master/lessons/files/fitbit_hybrid3_steps_df.csv)\n",
    "<img src=\"https://raw.githubusercontent.com/gsprint23/aha/master/lessons/figures/ADM_hybrid3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_bin: 0.50 p_VC: 0.93 p_critical: 0.71 is_sig: 1\n"
     ]
    }
   ],
   "source": [
    "fname = r\"files\\fitbit_hybrid3_data_steps_df.csv\"\n",
    "run_virtual_classifier(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid-Synthetic Profile 4\n",
    "[fitbit_hybrid4_steps_df.csv](https://raw.githubusercontent.com/gsprint23/aha/master/lessons/files/fitbit_hybrid4_steps_df.csv)\n",
    "<img src=\"https://raw.githubusercontent.com/gsprint23/aha/master/lessons/figures/ADM_hybrid4.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_bin: 0.50 p_VC: 1.00 p_critical: 0.71 is_sig: 1\n"
     ]
    }
   ],
   "source": [
    "fname = r\"files\\fitbit_hybrid4_data_steps_df.csv\"\n",
    "run_virtual_classifier(fname)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
